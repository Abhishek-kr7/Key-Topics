Hive
Duplicate Delete
Remove specific column from Hive table
Partition
Bucketing
External/Internal
Loading data /user/hive/warehouse/data
Loading data from other location without removing the source file and moving it to warehouse dir --> Alter table Add Partition.
Hive VS Impala
Date Function in Hive.

Convert string to Date in hive

unix_timestamp(string date, string pattern)
from_unixtime(unix_timestamp('12-03-2010' , 'dd-MM-yyyy'))  --This will be string only

use to_date
cast(to_date(from_unixtime(unix_timestamp('12-03-2010' , 'dd-MM-yyyy'))))

Delta Operation
Join
Scala
Exception handling

ACID - 
Atomicity
—Abort: If a transaction aborts, changes made to database are not visible.
—Commit: If a transaction commits, changes made are visible.

Consistency
Database should remain consistence, before and after Transactions
Total before T occurs = 500 + 200 = 700.
Total after T occurs = 400 + 300 = 700.

Isolation
Each Transaction should run in isolation
This property ensures that multiple transactions can occur concurrently without leading to the inconsistency of database state.

Durability:
This property ensures that once the transaction has completed execution,
the updates and modifications to the database are stored in and written to disk and they persist even if a system failure occurs.

SELECT 
	[every column], count(*) 
FROM ( 
	SELECT [every column], * 
	FROM table 
	DISTRIBUTE BY [every column] 
	HAVING count(*) > 1  
) t;


HBase


Spark
spark = SparkSession.builder \
    .config('spark.sql.shuffle.partitions', '100') \
    .getOrCreate()

spark.conf.set("spark.dynamicAllocation.enabled", "true")
spark.conf.set("spark.executor.cores", 4)
spark.conf.set("spark.dynamicAllocation.minExecutors","1")
spark.conf.set("spark.dynamicAllocation.maxExecutors","5")

Shuffle
What is Spark Shuffle?
Shuffling is a mechanism Spark uses to redistribute the data across different executors and even across machines. 
Spark shuffling triggers when we perform certain transformation operations like gropByKey(), reducebyKey(), join() on RDD and DataFrame.
Spark Shuffle is an expensive operation since it involves the following
Disk I/O
Involves data serialization and deserialization
Network I/O

Repartition
In Spark or PySpark repartition is used to increase or decrease the RDD, DataFrame, Dataset partitions
It is a costly operation as it shuffle the data across many partitions.

It can be used to Increase or decrease th number of partition
spark.sql.shuffle.partitions=500

Coalesce
Unlike repartition coalesce is used to decrease the number of partitions and not to increase the partition.
This is an optimized or improved version of repartition which can be used to reduce the number of partition and where the movement of data will be 
low across the partitions.
val df3 = df.coalesce(2)
println(df3.rdd.partitions.length)

NULLIF
Used to replace with specific value with NULL if column contains matches with the a given value or keyword
NULLIF(expression1, expression2 )
MAKE column as NULL if expresion1 matches expression2

REGEX
Used to search and modify specific regular expression
SELECT DisplayName
   FROM Players
   WHERE REGEXP_LIKE(DisplayName, '^Ste.*');


Executor memory decision
================================================

1.Tiny Executor --> number of executor same as number of Core (Not Suggested)
2.Fat Executor --> Number of executor same as number of Core.(Not Suggested)
3.Combination of Tiny and Fat.(Good to Use)

Calculation of executor's memory
—executor-memory = memory per node / num of executors per node

Let's Consider
32 Core
100GB Memory

1.Tiny
[Number of executor same as number of Core]
Core = 32
Total executor = same as core means total 32 executors
Executor memory = memory / executor ==> 100/32 --> 3GB per executor
 
2.Fat
[Number of executor same as number of Node]
core = 32
executor = 1
executor memory = memory/executor ==> 100/1 ---> 100GB 

3.Combination of Tiny Executor and Fat Executor

Note: 5 core per executor is a good choice. We will consider 5 core here for each executors

Core = 32 
take 1 core for Hadoop/Yarn daemons
Remaining Core = 32 -1 = 31 core.
Total Executor =  31/5 => 6 Executors
Keep 1 aside for application Manager.
Remaining executor = 6-1 => 5 executor
Executor memory => memory/executor ==> 100/5 => 20GB (exclude 7% for heap overhead)
Actual Executor memory = 20 - (7%20) = 20 - 1.4 => 18.6 or 18GB per executor. 


Schema

Cache
Cache is one of the optimization technique in Spark to improve the data processing speed and performance where we can keep the data in memory
which will be used multiple times in the processing operation.
Cache by default used to save the in memory

df.cache()

Persist()
Similar to Cache, persist is also used to save the data in memory but along with that you can use the user defined way of saving it as well
Storage option

MEMORY_ONLY 
MEMORY_ONLY – This is the default behavior of the RDD cache() method and stores the RDD or DataFrame as deserialized objects to JVM memory. 
When there is no enough memory available it will not save DataFrame of some partitions and these will be re-computed as and when required. This takes more memory. 

df.persist(StorageLevel.MEMORY_ONLY)

MEMORY_ONLY_SER 
This is the same as MEMORY_ONLY but the difference being it stores RDD as serialized objects to JVM memory. 
It takes lesser memory (space-efficient) then MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.

ndf = df.persist(StorageLevel.MEMORY_ONLY_SER)

MEMORY_AND_DISK 
MEMORY_AND_DISK_SER – This is same as MEMORY_AND_DISK storage level difference being it serializes the DataFrame objects in memory and on disk when space not available.

ndf = df.persist(StorageLevel.MEMORY_AND_DISK)

DISK_ONLY – In this storage level, DataFrame is stored only on disk and the CPU computation time is high as I/O involved.

ndf = df.persist(StorageLevel.DISK_ONLY)

Unpersist()
ndf.unpersist()


Type and difference in file format

Avro:
Since it’s a row based format, it’s better to use when all fields needs to be accessed
Files support block compression and are splittable
Suitable for write intensive operation
If your use case typically scans or retrieves all of the fields in a row in each query, Avro is usually the best choice.

Parquet:
Apache Parquet, on the other hand, is a free and open-source column-oriented data storage 
format of the Apache Hadoop ecosystem. 
It is similar to the other columnar-storage file formats available in Hadoop namely RCFile and ORC. 
Since it’s a column based format, it’s better to use when you only need to access specific fields.
Each data file contains the values for a set of rows.
Can’t be written from streaming data since it needs to wait for blocks to get finished. However, this will work using micro-batch (eg Apache Spark).
Suitable for data exploration — read intensive, complex or analytical querying, low latency data
If your dataset has many columns, and your use case typically involves working with a subset of those columns rather than entire records, 
Parquet is optimized for that kind of work.


Skewness in Spark
one process is hang in spark and other executed 

Parallelism in Kafka
trending Tweet count from Kafka

How data will be differ in Flatmap and Map
Spark Map function takes one element as input process it according to custom code (specified by the developer) and returns one element at a time. 
Map transforms an RDD of length N into another RDD of length N. The input and output RDDs will typically have the same number of records.

A FlatMap function takes one element as input process it according to custom code (specified by the developer) and returns 0 or more element at a time. 
flatMap() transforms an RDD of length N into another RDD of length M.

Data
Abc def ghi
jkl mno pqr
stu vwx yza

val splitdata = data.flatMap(line => line.split(" "));  
Array(abc,def,ghi,jkl,mno,....)

val mapdata = splitdata.map(word => (word,1)); 
Array((abc,1),(def,1),(ghi,1)....)

shuffle default in spark or not?
default and is set to 200 partitions

default value of shuffle?
200


Spark Broadcast Variables
Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. 
Instead of sending this data along with every task, spark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.


traits in scala


connect api vs consumer api





Dataframe

Explode

explode – spark explode array or map column to rows

val arrayData = Seq(
    Row("James",List("Java","Scala"),Map("hair"->"black","eye"->"brown")),
    Row("Michael",List("Spark","Java",null),Map("hair"->"brown","eye"->null)),
    Row("Robert",List("CSharp",""),Map("hair"->"red","eye"->"")),
    Row("Washington",null,null),
    Row("Jefferson",List(),Map())
    )
   

    val arraySchema = new StructType()
      .add("name",StringType)
      .add("knownLanguages", ArrayType(StringType))
      .add("properties", MapType(StringType,StringType))

    val df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData),arraySchema)
    df.printSchema()
    df.show(false)
    

Spark function explode(e: Column) is used to explode or create array or map columns to rows. When an array is passed to this function, 
it creates a new default column “col1” and it contains all array elements. When a map is passed, 
it creates two new columns one for key and one for value and each element in map split into the row.  
  
  
explode – array column example
df.select($"name",explode($"knownLanguages"))
      .show(false)

+-------+------+
|name   |col   |
+-------+------+
|James  |Java  |
|James  |Scala |
|Michael|Spark |
|Michael|Java  |
|Michael|null  |
|Robert |CSharp|
|Robert |      |
+-------+------+

explode – map column example

df.select($"name",explode($"properties"))
  .show(false)

+-------+----+-----+
|name   |key |value|
+-------+----+-----+
|James  |hair|black|
|James  |eye |brown|
|Michael|hair|brown|
|Michael|eye |null |
|Robert |hair|red  |
|Robert |eye |     |
+-------+----+-----+

show(false)
This will show the full column content without truncating. like


df.show()
12-30-10:00:....
12-30-11:00:....
12-30-12:00:....

df.show(false)
12-30-10:00:55:45
12-30-11:00:23:34
12-30-12:00:12:23

isIn

Modes in option(Permissive)

Column with corrupt Value
There are ways to handle
we can place null value if any corrupt value occurs --- .option("mode","PERMISSIVE")
we can place the bad record in a file if occurs --- .option("badRecordsPath","/tmp/badRecordsPath")
we can drop the bad record  ---- .option("mode", "DROPMALFORMED")
We can stop processing if any corrup value occurs ---- .option("mode", "FAILFAST")

Word Count in Pyspark
import sys
 
from pyspark import SparkContext, SparkConf
 
if __name__ == "__main__":
	
	# create Spark context with necessary configuration
	sc = SparkContext("local","PySpark Word Count Exmaple")
	
	# read data from text file and split each line into words
	words = sc.textFile("D:/workspace/spark/input.txt").flatMap(lambda line: line.split(" "))
	
	# count the occurrence of each word
	wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)
	
	# save the counts to output
	wordCounts.saveAsTextFile("D:/workspace/spark/output/")


Kafka



Sqoop


Cluster Specifications	   Stage	Prod
Cluster Name	           TCDH2S_New	PCDH2S_New
Number of nodes 27 455	   27	        455
HDFS Size	           403 TB	21.12 PB
Number of cores            1160	        29032
Total Memory	           11 TB	174 TB
CDH Version                6.3.3	6.3.3
Spark version	           2.4.0	2.4.0
Hive Version               1.1.0	1.1.0
Impala Version	           2.12	        2.12
Scala version              2.11.2       2.11.2
Python                     3.7.3        3.7.3


Cluster Specifications Stage Prod
Cluster Name
TCDH2S_New
PCDH2S_New
Number of nodes 27 455
HDFS Size
403 TB
21.12 PB
Number of cores 1160 29032
Total Memory
11 TB
174 TB
CDH Version 5.16.2 5.16.2
Spark version
2.4.0
2.4.0
Hive Version 1.1.0 1.1.0
Impala Version
2.12
2.12



If i have cluster of 5 nodes, each node having 1gb ram, now if my data file is 10gb distributed in all 5 nodes, let say 2gb in each node


Node = 5
Memory Per Node = 1Gb
Data per Node = 2Gb

Since memory per node is less than data pernode , we can try processing data in chunks.

Lets divide data in 4 partition while creating RDD

rdd = sc.textFile("data/location/2gb.txt",4)

rdd.persist(storage_level, 'MEMORY_AND_DISK_ONLY')

In this way, first 500mb data will get process and then spilled on the disk.
Then next 500mb will get process and spilled on the disk.

Same will happen on each node.
This way 20GB data will get process with the Help of Partitioning file.

But Since we are not including any memory for hadoop/yarn daemon, chances are it may fail.
Processing will be very slow.


groupByKey() is just to group your dataset based on a key. It will result in data shuffling when RDD is not already partitioned.
reduceByKey() is something like grouping + aggregation. We can say reduceBykey() equvelent to dataset.group(...).reduce(...). 
It will shuffle less data unlike groupByKey().
aggregateByKey() is logically same as reduceByKey() but it lets you return result in different type. In another words, 
it lets you have a input as type x and aggregate result as type y. For example (1,2),(1,4) as input and (1,"six") as output. 
It also takes zero-value that will be applied at the beginning of each key.


Narrow transformation — In Narrow transformation, all the elements that are required to compute the records in single partition 
live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().

Wide transformation — In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent
RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey and reducebyKey.

beeline -u $hiveurl -e "show columns from etl_enc.clearing_chip_detail;" | grep -v "field" | grep -v "-" > fieldname.txt

